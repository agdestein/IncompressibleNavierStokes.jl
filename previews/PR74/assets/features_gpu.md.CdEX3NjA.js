import{_ as e,c as s,o as a,a7 as t}from"./chunks/framework.DV1fsDIA.js";const g=JSON.parse('{"title":"GPU Support","description":"","frontmatter":{},"headers":[],"relativePath":"features/gpu.md","filePath":"features/gpu.md","lastUpdated":null}'),i={name:"features/gpu.md"},l=t(`<h1 id="GPU-Support" tabindex="-1">GPU Support <a class="header-anchor" href="#GPU-Support" aria-label="Permalink to &quot;GPU Support {#GPU-Support}&quot;">â€‹</a></h1><p>IncompressibleNavierStokes supports various array types. The desired array type only has to be passed to the <a href="/IncompressibleNavierStokes.jl/previews/PR74/examples/generated/prioranalysis#Setup"><code>Setup</code></a> function:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> CUDA</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">setup </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> Setup</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; kwargs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, ArrayType </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> CuArray)</span></span></code></pre></div><p>All operators have been made are backend agnostic by using <a href="https://github.com/JuliaGPU/KernelAbstractions.jl/" target="_blank" rel="noreferrer">KernelAbstractions.jl</a>. Even if a GPU is not available, the operators are multithreaded if Julia is started with multiple threads (e.g. <code>julia -t 4</code>)</p><ul><li><p>This has been tested with CUDA compatible GPUs.</p></li><li><p>This has not been tested with other GPU interfaces, such as</p><ul><li><p><a href="https://github.com/JuliaGPU/AMDGPU.jl" target="_blank" rel="noreferrer">AMDGPU.jl</a></p></li><li><p><a href="https://github.com/JuliaGPU/Metal.jl" target="_blank" rel="noreferrer">Metal.jl</a></p></li><li><p><a href="https://github.com/JuliaGPU/oneAPI.jl" target="_blank" rel="noreferrer">oneAPI.jl</a></p></li></ul><p>If they start supporting sparse matrices and fast Fourier transforms they could also be used.</p></li></ul><div class="tip custom-block"><p class="custom-block-title"><code>psolver_direct</code> on CUDA</p><p>To use a specialized linear solver for CUDA, make sure to install and <code>using</code> CUDA.jl and CUDSS.jl. Then <code>psolver_direct</code> will automatically use the CUDSS solver.</p></div>`,6),r=[l];function p(n,o,h,d,c,u){return a(),s("div",null,r)}const _=e(i,[["render",p]]);export{g as __pageData,_ as default};
